iris_df<-read.csv("/Users/Mansoor/Documents/DS Lancaster/R programming/python_finalproject/datasets for project/Iris_names.csv", header = TRUE)
iris_df$species<-factor(iris_df$species, levels = c("Iris-setosa","Iris-versicolor","Iris-virginica"),labels = c(0,1,2))
train_idx<-createDataPartition(iris_df$species,p=0.7,list = FALSE)
train_df<-iris_df[train_idx,]
test_df<-iris_df[-train_idx,]
x_train<-train_df %>% select(-species) %>% scale()
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
x_train<-train_df %>% select(-species) %>% scale()
y_train<-to_categorical(train_df$species,3)
library(tensorflow)
y_train<-to_categorical(train_df$species,3)
install.packages("forecast")
library("forecast", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("leaps")
install.packages("tsintermittent")
install.packages("hts")
install.packages("Mcomp")
install.packages("fpp")
install.packages("devtools")
library("keras", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
remove.packages("keras")
library("tensorflow", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
remove.packages("tensorflow")
library(forecast,MASS,Leaps,tsintermittent,hts,MComp,fpp,devtool)
library(forecast,MASS,Leaps,hts,MComp,fpp,devtool)
forecast,MASS,Leaps,tsintermittent,hts,MComp,fpp,devtool)
install.packages("tsutils")
#forecast,MASS,Leaps,tsintermittent,hts,MComp,fpp,devtool
library(tsutils)
library(forecast)
rm(list=ls())
data<-read.table('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 2/sales.txt')
head(data)
data <- ts(data, frequency = 12, start = c(2011,1))
frequency(data)
frequency(data)
plot(data)
data_cma <- cmav(data, ma = 12, fill = FALSE)
#Plot the original time series in blue
plot(data, col="blue")
#Plot the CMA(12) in red
lines(data_cma, col = "red")
seasplot(data)
data_seas_index <- seasplot(data)$season
data_seas_index
decomposition <- decomp(data,decomposition = "multiplicative",outplot = TRUE)
str(decomposition)
decomposition <- decomp(data,decomposition = "additive",outplot = TRUE)
#Extract the Trend
decomposition$trend
#Extract the Trend
decomposition$trend
#Extract the Seasonality
decomposition$season
#Multiply the Trend and Seasonality
regular_components <- decomposition$trend * decomposition$season
mmm_errors <- data - regular_components
decomposition$irregular - mmm_errors
mmm_errors <- data - regular_components
decomposition$irregular - mmm_errors
decomposition$irregular-mmm_errors
mmm_errors <- data - regular_components
decomposition$irregular-mmm_errors
mmm_errors <- data-regular_components
#Multiply the Trend and Seasonality
regular_components <- decomposition$trend * decomposition$season
#Extract the Seasonality
decomposition$season
#Extract the Trend
decomposition$trend
str(decomposition)
decomposition <- decomp(data,decomposition = "multiplicative",outplot = TRUE)
str(decomposition)
#Extract the Trend
decomposition$trend
#Extract the Seasonality
decomposition$season
#Multiply the Trend and Seasonality
regular_components <- decomposition$trend * decomposition$season
mmm_errors <- data-regular_components
decomposition$irregular-mmm_errors
library("forecast", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("smooth", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library(forecast)
library(smooth)
library(tsutils)
workshop3<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/workshop.csv')
workshop3<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/workshop3.csv')
workshop3<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/workshop3.csv')
workshop3
medium_noise <- ts(Workshop3[,1],frequency=12)
medium_noise <- ts(workshop3[,1],frequency=12)
plot(medium_noise)
decomp(medium_noise, decomposition="additive", outplot=TRUE)
decomp(medium_noise,decomposition="multiplicative", outplot=TRUE)
decomp(medium_noise,decomposition="multiplicative", outplot=TRUE)
decomp(medium_noise, decomposition="additive", outplot=TRUE)
decomp(medium_noise,decomposition="multiplicative", outplot=TRUE)
# Find the total number of observations
medium_noise_length <- length(medium_noise)
# Write down size of train set
train_length <- 36
# And the forecasting horizon
h <- 12
# Create the training set
train <- ts(medium_noise[1:train_length], frequency=12)
# Create the test set
test <- ts(medium_noise[(train_length+1):medium_noise_length],frequency=12)
SMA <- ma(train, order=3, centre=FALSE)
# Firstly we get rid of NA (‘‘Not Assigned’’) values:
SMA_no_NAs <- SMA[!is.na(SMA)]
# Then form a forecast:
SMA3_forecast <- ts(rep(SMA_no_NAs[length(SMA_no_NAs)],12), frequency=12)
SMA3_errors <- test - SMA3_forecast
SMA3_ME <- mean(SMA3_errors)
SMA3_ME <- mean(SMA3_errors)
SMA3_MSE <- mean(SMA3_errors ^ 2)
SMA3_MAE <- mean(abs(SMA3_errors))
SMA3_MAPE <- 100 * mean(abs(SMA3_errors)/test)
naive_method <- naive(train, h=h)
naive_forecast <- method$mean
library(methods)
naive_forecast <- method$mean
ETS_ANN_0.15 <- ets(train, model="ANN", alpha=0.15)
ETS_ANN_0.15
ETS_ANN_0.15_forecast <- forecast(ETS_ANN_0.15, h=h)$mean
plot(forecast(ETS_ANN_0.15, h=h))
ES_ANN_0.15 <- es(train, "ANN", persistence=0.15, h=h)
ES_ANN_0.15
ES_ANN_opt <- es(train, "ANN", h=h, silent="a")
# Fit SES with fixed initial seed
es_ANN_initial_1 <- es(medium_noise, model="ANN", initial=medium_noise[1],
h=h, holdout=TRUE)
es_ANN_initial_1$accuracy
# Fit SES with optimised Seed
es_ANN_opt <- es(medium_noise, model="ANN", h=h, holdout=TRUE)
es_ANN_opt$accuracy
# Fit SES with optimised Seed
medium_noise_naive <- es(medium_noise, model="ANN", persistence=1,
h=h, holdout=TRUE)
trend_data<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/trend_data.csv')
trend_data<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/trend and trend seasonal data/trend_data.csv')
plot(trend_data)
trend_data_ts<-ts(trend_data)
plot(trend_data_ts)
trend_data_length <- length(trend_data)
# Split into training and test
trend_data_train <- ts(trend_data[1:36], frequency = 12)
# Split into training and test
trend_data_train <- ts(trend_data[1:36], frequency = 12)
trend_data<-read.csv('/Users/Mansoor/Documents/DS Lancaster/lent term/Forecasting/lecture 3/trend and trend seasonal data/trend_data.csv', header = TRUE)
trend_data_length <- length(trend_data)
# Split into training and test
trend_data_train <- ts(trend_data[1:36], frequency = 12)
install.packages("BRugs")
library(BRugs)
install.packages("BRugs")
install.packages("BRugs")
library(BRugs)
install.packages("installr"); library(installr) # install+load installr
install.packages('R2OpenBUGS',type = 'source')
library("R2OpenBUGS", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library(R2OpenBUGS)
data(mammalsleep)
data(mammalsleep)
attach(mammalsleep)
library(mice)
data(mammalsleep)
mammalsleep[,c(2:3,7:8)]=log(mammalsleep[,c(2:3,7:8)])
names(mammalsleep)=c("species", "lbw", "lbrw", "sws", "ps",
"ts", "lmls", "lgt", "pi", "sei", "odi")
M2=as.data.frame(mammalsleep[,2:9])
x=M2$lbw
y=M2$ps
z=M2$lbrw
N=length(x)
BRugsFit("bmodel0.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","Y"),numChains =2)
R2OpenBUGS("bmodel2.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","I","X","Y","sigma2"),numChains =2)
library(R2OpenBUGS) # alternative
R2OpenBUGS("bmodel2.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","I","X","Y","sigma2"),numChains =2)
R2OpenBUGSfit("bmodel2.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","I","X","Y","sigma2"),numChains =2)
library(BRugs)
library(R2OpenBUGS) # alternative
library(BRugs)
BRugsFit("bmodel0.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","Y"),numChains =2)
BRugsFit("bmodel0.txt", data=list(N=N,X=x,Y=y,Z=z),
para=c("b","sigma","Y"),numChains =2)
library(mice)
library(VIM)
library(BRugs)  #Ensure that OpenBUGS is installed
install.packages("OpenBUGS")
library(BRugs)  #Ensure that OpenBUGS is installed
library("R2OpenBUGS", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("mice", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library(mice)
library(VIM)
library(BRugs)  #Ensure that OpenBUGS is installed
###Lecture 5
library(R2OpenBUGS)
library(BRugs)  #Ensure that OpenBUGS is installed
install.packages("BRugs")
library(BRugs)  #Ensure that OpenBUGS is installed
library(mice)
\doc\JSScode.R
knitr::opts_chunk$set(echo = TRUE)
print('I am ur dsfsf')
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(VIM)
library(corrplot)
library(mice)
library(xtable)
set.seed(1234)
DM = read.csv("diabetes_C.csv")
setwd("~/Documents/GitHub/missing_data")
library(VIM)
library(corrplot)
library(mice)
library(xtable)
set.seed(1234)
DM = read.csv("diabetes_C.csv")
# dataset description with column names in report
DM = DM[,-c(2)]
# removing stabalized gluscose from dataset
#DM[,14] = as.numeric(c(DM[,4]>7))
#DM = DM[,-4]
names(DM)
DM
#DM[,c(5,8)] = as.factor(DM[,c(5,8)])
#head(DM)
aggr(DM, numbers=TRUE, main= "Extent of missing values in each variable of Diabetic")
matrixplot(DM) # no relation found
#names(DM)
head(DM)
C=cor(DM[,-c(6,9)],use = "pairwise")
corrplot(C,type = "upper", order = "hclust",
title = "Correlation Plot of Diabetes Dataset",method = "number")
library(VIM)
library(corrplot)
library(mice)
library(xtable)
set.seed(1234)
DM = read.csv("diabetes_C.csv")
# dataset description with column names in report
DM = DM[,-c(2)]
# removing stabalized gluscose from dataset
#DM[,14] = as.numeric(c(DM[,4]>7))
#DM = DM[,-4]
names(DM)
head(DM)
#DM[,c(5,8)] = as.factor(DM[,c(5,8)])
#head(DM)
aggr(DM, numbers=TRUE, main= "Extent of missing values in each variable of Diabetic dataset")
matrixplot(DM) # no relation found
#names(DM)
C=cor(DM[,-c(6,9)],use = "pairwise")
corrplot(C,type = "upper", order = "hclust",
title = "Correlation Plot of Diabetes Dataset",method = "number")
# complete case analysis
DM1 = na.omit(DM)
length(DM1[,1]) # 367 rows
length(DM[,1]) # 403 rows
names(DM1)
y=DM1[,4]
X=DM1[,-4]
Model1=lm(y~.,data=X)
summary(Model1)
Model2=step(Model1)
summary(Model1)
(summary(Model2))
complete_case<-summary(Model2)
complete_case
round(complete_case$coefficients[,1],3)
# coeefecients or estimates
round(complete_case$coefficients[,2],3)
# standard errors
#simple model
M.imp = mice(DM,method = "mean",m=20)
names(M.imp)
M.imp$imp
#simple model
M.imp = mice(DM,method = "mean",m=20)
names(M.imp)
M.imp$imp
y=DM[,4]
X=DM[,-4]
Model3=with(M.imp,lm(y~ratio+age+waist,data=X))
#Model3=step(Model3)
summary(Model3)
pool(Model3)
(summary(pool(Model3)))
mean_coef<-round(summary(pool(Model3)),2)
mean_coef[,1] # estimates
mean_coef[,2] # standard errors
xyplot(M.imp,glyhb~chol+hdl+ratio+age+height
+weight+frame+bp.1s+bp.1d+waist+hip,
main="Single Imputation")
# chained model
C.imp = mice(DM,m = 20)
com = complete(C.imp,"long")
com = complete(C.imp)
model = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+height+
weight+frame+bp.1s+bp.1d+waist+hip) )
summary(pool(model))
# chained model
C.imp = mice(DM,m = 20)
com = complete(C.imp,"long")
com = complete(C.imp)
model = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+height+
weight+frame+bp.1s+bp.1d+waist+hip) )
summary(pool(model))
model1 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+frame+bp.1s+bp.1d+waist+hip))
summary(pool(model1))
pool.compare(model,model1,method = "wald")
model2 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip))
summary(pool(model2))
pool.compare(model1,model2,method = "wald")
model3 = with(data = C.imp,exp =
lm(glyhb~hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip) )
summary(pool(model3))
pool.compare(model2,model3,method = "wald")
model4 = with(data = C.imp,exp = lm(glyhb~
hdl+ratio+
age+height+bp.1s+
bp.1d+waist) )
summary(pool(model4))
pool.compare(model3,model4,method = "wald")
model5 = with(data = C.imp,exp = lm(glyhb~hdl
+ratio+age+
height+bp.1d+waist) )
summary(pool(model5))
pool.compare(model4,model5,method = "wald")
model6 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
height+waist) )
summary(pool(model6))
pool.compare(model5,model6,method = "wald")
model7 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
waist) )
summary(pool(model7))
pool.compare(model6,model7,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8$analyses
names()
model1 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+frame+bp.1s+bp.1d+waist+hip))
summary(pool(model1))
pool.compare(model,model1,method = "wald")
model2 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip))
summary(pool(model2))
pool.compare(model1,model2,method = "wald")
model3 = with(data = C.imp,exp =
lm(glyhb~hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip) )
summary(pool(model3))
pool.compare(model2,model3,method = "wald")
model4 = with(data = C.imp,exp = lm(glyhb~
hdl+ratio+
age+height+bp.1s+
bp.1d+waist) )
summary(pool(model4))
pool.compare(model3,model4,method = "wald")
model5 = with(data = C.imp,exp = lm(glyhb~hdl
+ratio+age+
height+bp.1d+waist) )
summary(pool(model5))
pool.compare(model4,model5,method = "wald")
model6 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
height+waist) )
summary(pool(model6))
pool.compare(model5,model6,method = "wald")
model7 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
waist) )
summary(pool(model7))
pool.compare(model6,model7,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8$analyses
names()
model1 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+frame+bp.1s+bp.1d+waist+hip))
summary(pool(model1))
pool.compare(model,model1,method = "wald")
model2 = with(data = C.imp,exp =
lm(glyhb~chol+hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip))
summary(pool(model2))
pool.compare(model1,model2,method = "wald")
model3 = with(data = C.imp,exp =
lm(glyhb~hdl+ratio+age+
height+bp.1s+bp.1d+waist+hip) )
summary(pool(model3))
pool.compare(model2,model3,method = "wald")
model4 = with(data = C.imp,exp = lm(glyhb~
hdl+ratio+
age+height+bp.1s+
bp.1d+waist) )
summary(pool(model4))
pool.compare(model3,model4,method = "wald")
model5 = with(data = C.imp,exp = lm(glyhb~hdl
+ratio+age+
height+bp.1d+waist) )
summary(pool(model5))
pool.compare(model4,model5,method = "wald")
model6 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
height+waist) )
summary(pool(model6))
pool.compare(model5,model6,method = "wald")
model7 = with(data = C.imp,exp = lm(glyhb~hdl+
ratio+age+
waist) )
summary(pool(model7))
pool.compare(model6,model7,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8 = with(data = C.imp,exp = lm(glyhb~
ratio+age+
waist) )
summary(pool(model8))
pool.compare(model7,model8,method = "wald")
model8$analyses
#names()
#?pool.compare
#x11()
multiple_coef<-round(summary(pool(model8)),2)
multiple_coef[,1] # estimates
multiple_coef[,2] # standard error
pool.compare(model3,model8,method = "wald")
# getting p value not significant
#so we are going for the simple model8
par(mfrow=c(2,1))
plot(C.imp, main='Convergence plot
of imputed diabetes data') # converegence plot
par(mfrow=c(7,2))
attach(model8)
par(mfrow=c(4,4))
densityplot(C.imp, ~glyhb,main =
"Density plot of Imputations for glyhb")
densityplot(C.imp, ~height,main =
"Density plot of Imputations for height")
densityplot(C.imp, ~frame,main =
"Density plot of Imputations for frame")
densityplot(C.imp, ~bp.1d,main =
"Density plot of Imputations for bp.1d")
x11()
stripplot(C.imp, main =
"Stripplot of all
imputations in diabetes dataset") # every single imputation
xyplot(C.imp,glyhb~chol+hdl+ratio
+age+height+weight+frame+bp.1s+bp.1d+waist+hip,
main="multiple chained imputations")
bwplot(C.imp,main="Box and whisker plot of
observed and imputed data in diabetes dataset")
attach(model8)
par(mfrow=c(4,4))
densityplot(C.imp, ~glyhb,main =
"Density plot of Imputations for glyhb")
densityplot(C.imp, ~height,main =
"Density plot of Imputations for height")
densityplot(C.imp, ~frame,main =
"Density plot of Imputations for frame")
densityplot(C.imp, ~bp.1d,main =
"Density plot of Imputations for bp.1d")
#x11()
stripplot(C.imp, main =
"Stripplot of all
imputations in diabetes dataset") # every single imputation
xyplot(C.imp,glyhb~chol+hdl+ratio
+age+height+weight+frame+bp.1s+bp.1d+waist+hip,
main="multiple chained imputations")
bwplot(C.imp,main="Box and whisker plot of
observed and imputed data in diabetes dataset")
